{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {"name": "python3", "display_name": "Python 3", "language": "python"},
    "language_info": {
      "name": "python",
      "version": "3.x",
      "mimetype": "text/x-python",
      "codemirror_mode": {"name": "ipython", "version": 3},
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "cells": [
    {"cell_type": "markdown", "metadata": {}, "source": "# EN3150 — Assignment 01 Notebook (Refreshed)\n**Learning from Data and Related Challenges & Linear Models for Regression**  \n**University of Moratuwa — Department of Electronic & Telecommunication Engineering**\n\n**Prepared notebook template:** <auto-generated today>\n\nThis refreshed version fixes earlier minor issues and keeps plots simple and compatible."},

    {"cell_type": "markdown", "metadata": {}, "source": "**Student Name:** _<type here>_  \n**Index Number:** _<type here>_  \n"},

    {"cell_type": "markdown", "metadata": {}, "source": "## Setup"},
    {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": "# Imports\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Ensure inline plotting\n%matplotlib inline\n\n# (Optional) Set a base random seed for reproducibility of demo plots\nnp.random.seed(0)\n\n# ---------- Helper functions ----------\ndef linear_fit(x, y):\n    \"\"\"Return slope and intercept for y ≈ m x + c using least squares.\"\"\"\n    x = np.asarray(x, dtype=float)\n    y = np.asarray(y, dtype=float)\n    m, c = np.polyfit(x, y, 1)\n    return m, c\n\ndef predict_line(x, m, c):\n    return m * np.asarray(x, dtype=float) + c\n\ndef robust_loss_per_sample(y_true, y_pred, beta):\n    \"\"\"Compute per-sample robust loss: r^2 / (r^2 + beta^2).\"\"\"\n    y_true = np.asarray(y_true, dtype=float)\n    y_pred = np.asarray(y_pred, dtype=float)\n    r2 = (y_true - y_pred)**2\n    beta2 = (beta**2)\n    return r2 / (r2 + beta2)\n\ndef robust_loss_mean(y_true, y_pred, beta):\n    return robust_loss_per_sample(y_true, y_pred, beta).mean()\n\ndef mse(y_true, y_pred):\n    y_true = np.asarray(y_true, dtype=float)\n    y_pred = np.asarray(y_pred, dtype=float)\n    return np.mean((y_true - y_pred)**2)\n\ndef bce_for_y_equals_1(yhat, eps=1e-12):\n    # Clip to avoid log(0) or log(1) producing -inf\n    yhat = np.clip(np.asarray(yhat, dtype=float), eps, 1.0 - eps)\n    return -np.log(yhat)\n\n# Fallback scaling utilities (no sklearn required)\ndef standard_scale(x):\n    x = np.asarray(x, dtype=float)\n    std = x.std(ddof=0)\n    return (x - x.mean()) / (std if std != 0 else 1.0)\n\ndef minmax_scale(x):\n    x = np.asarray(x, dtype=float)\n    xmin, xmax = x.min(), x.max()\n    return (x - xmin) / (xmax - xmin) if xmax != xmin else np.zeros_like(x)\n\ndef maxabs_scale(x):\n    x = np.asarray(x, dtype=float)\n    denom = np.max(np.abs(x))\n    return x / denom if denom != 0 else np.zeros_like(x)\n"},

    {"cell_type": "markdown", "metadata": {}, "source": "## 1) Linear Regression Impact on Outliers"},
    {"cell_type": "markdown", "metadata": {}, "source": "### Task 1–2: Load dataset (Table 1), fit linear regression, and plot\nData from the assignment (x, y):"},
    {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": "# Table 1 data (i from 1..10)\nx = np.array([0,1,2,3,4,5,6,7,8,9], dtype=float)\ny = np.array([20.26, 5.61, 3.14, -30.00, -40.00, -8.13, -11.73, -16.08, -19.95, -24.03], dtype=float)\n\n# Least squares linear regression\nm_hat, c_hat = linear_fit(x, y)\nprint(f\"Learned linear model (Task 2): y = {m_hat:.4f} x + {c_hat:.4f}\")\n\n# Plot scatter + fitted line\nxx = np.linspace(x.min()-0.5, x.max()+0.5, 200)\nyy = predict_line(xx, m_hat, c_hat)\n\nplt.figure(figsize=(7,5))\nplt.scatter(x, y, s=35, label='Data points')\nplt.plot(xx, yy, linewidth=2, label='Fitted line')\nplt.title('Linear Regression on Given Data')\nplt.xlabel('x'); plt.ylabel('y')\nplt.legend(); plt.grid(True)\nplt.show()\n"},

    {"cell_type": "markdown", "metadata": {}, "source": "### Task 3–4: Robust loss for two given models\nTwo models:\n- **Model 1**: $y = -4x + 12$  \n- **Model 2**: $y = -3.55x + 3.91$ (stated as the learned model in Task 2)\n\nCompute per-sample and mean robust loss $L(\\theta, \\beta)$ for $\\beta \\in \\{1, 10^{-6}, 10^{3}\\}$."},
    {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": "# Define the two candidate models\nm1, c1 = -4.0, 12.0\nm2, c2 = -3.55, 3.91\n\ny1 = predict_line(x, m1, c1)\ny2 = predict_line(x, m2, c2)\n\nbetas = [1.0, 1e-6, 1e3]\n\nrows = []\nfor model_name, yp in [('Model 1', y1), ('Model 2', y2)]:\n    for b in betas:\n        per = robust_loss_per_sample(y, yp, b)\n        rows.append({'Model': model_name, 'beta': b, 'L_mean': per.mean()})\nresults_df = pd.DataFrame(rows)\nresults_df\n"},

    {"cell_type": "markdown", "metadata": {}, "source": "(Optional) Detailed per-sample table — uncomment in the next cell to view."},
    {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": "# details = []\n# for model_name, yp in [('Model 1', y1), ('Model 2', y2)]:\n#     for b in betas:\n#         per = robust_loss_per_sample(y, yp, b)\n#         for i, val in enumerate(per, start=1):\n#             details.append({'Model': model_name, 'beta': b, 'i': i, 'x': x[i-1], 'y': y[i-1], 'loss_i': val})\n# details_df = pd.DataFrame(details)\n# details_df.head(20)\n"},

    {"cell_type": "markdown", "metadata": {}, "source": "### (Optional) Visual: Robust loss vs. residual magnitude\nCompare how MSE (unbounded) and the robust loss $\\frac{r^2}{r^2+\\beta^2}$ (bounded by 1) behave."},
    {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": "r = np.linspace(0, 100, 400)\nmse_curve = (r**2)\nbeta_demo = 1.0\nrobust_curve = (r**2) / (r**2 + beta_demo**2)\n\nplt.figure(figsize=(7,5))\nplt.plot(r, mse_curve, label='MSE loss (r^2)')\nplt.plot(r, robust_curve, label=f'Robust loss (beta={beta_demo})')\nplt.xlabel('Residual magnitude |r|'); plt.ylabel('Loss value')\nplt.title('Loss vs Residual Magnitude')\nplt.legend(); plt.grid(True)\nplt.show()\n"},

    {"cell_type": "markdown", "metadata": {}, "source": "### Task 5–6: Choose suitable $\\beta$ and the better model\nUse the table above and your understanding to justify:\n- A suitable $\\beta$ to mitigate outliers (hint: too small ≈ indicator loss; too large ≈ MSE).\n- Which model (1 or 2) is preferred under the chosen $\\beta$.\n\n**Your justification (write in your own words):**"},
    {"cell_type": "markdown", "metadata": {}, "source": "- **Chosen β:** _<type here>_  \n- **Reasoning:** _<explain why this β balances outlier suppression vs. inlier sensitivity>_  \n- **Selected model:** _<Model 1 or Model 2>_  \n- **Reasoning:** _<compare L_mean values at your β and discuss fit to data>_  \n"},

    {"cell_type": "markdown", "metadata": {}, "source": "### Task 7: How does this robust estimator reduce the impact of outliers?\n**Your explanation (own words):**  \n_Hint: The loss saturates near 1 for large residuals, capping the contribution of outliers to the total loss, unlike MSE which grows without bound._"},

    {"cell_type": "markdown", "metadata": {}, "source": "### Task 8: Another robust loss you could use\nExamples include **Huber**, **Tukey’s biweight**, **Cauchy**, **Geman–McClure**.  \nBriefly describe one and why it helps."},

    {"cell_type": "markdown", "metadata": {}, "source": "## 2) Loss Functions — Linear vs Logistic Regression"},
    {"cell_type": "markdown", "metadata": {}, "source": "### Task 1: Fill the table (y=1), compute MSE and BCE, and plot"},
    {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": "y_true = 1.0\nyhat = np.array([0.005, 0.01, 0.05, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0], dtype=float)\n\nmse_vals = (y_true - yhat)**2\nbce_vals = bce_for_y_equals_1(yhat)\n\ntable_df = pd.DataFrame({\n    'True y': [1]*len(yhat),\n    'Prediction y_hat': yhat,\n    'MSE': mse_vals,\n    'BCE': bce_vals\n})\ntable_df\n"},
    {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": "# Plot MSE vs y_hat\nplt.figure(figsize=(6,4))\nplt.plot(yhat, mse_vals, marker='o')\nplt.title('MSE for y=1 vs prediction y_hat')\nplt.xlabel('y_hat'); plt.ylabel('MSE')\nplt.grid(True)\nplt.show()\n\n# Plot BCE vs y_hat\nplt.figure(figsize=(6,4))\nplt.plot(yhat, bce_vals, marker='o')\nplt.title('BCE for y=1 vs prediction y_hat')\nplt.xlabel('y_hat'); plt.ylabel('BCE')\nplt.grid(True)\nplt.show()\n"},
    {"cell_type": "markdown", "metadata": {}, "source": "### Task 2: Which loss for which application?\n- **Application 1 (continuous target, Linear Regression):** Prefer **MSE** (matches Gaussian noise assumption and penalizes squared deviations).\n- **Application 2 (binary target, Logistic Regression):** Prefer **BCE** (derived from Bernoulli likelihood; strongly penalizes confident wrong predictions and supports probability outputs).\n\nWrite a short justification in your own words below."},
    {"cell_type": "markdown", "metadata": {}, "source": "**Your justification:** _<type here>_"},

    {"cell_type": "markdown", "metadata": {}, "source": "## 3) Data Pre-processing — Generate Features & Choose Scaling"},
    {"cell_type": "markdown", "metadata": {}, "source": "### Task 1: Generate Feature 1 (sparse) and Feature 2 (Gaussian-like noise)\n> **Enter your index number** below (digits only, no leading zeros). The code reproduces Listing 1 logic and then applies three scaling methods. Choose one suitable method for each feature and justify."},
    {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": "# >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n# Enter your index number (digits only, e.g., 220123A -> 220123) — REQUIRED\nindex_no = 123456  # <-- change this\n# <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n\n# Listing 1 (adapted)\nsignal_length = 100\nnum_nonzero = 10\n\ndef generate_signal(signal_length, num_nonzero, seed=None):\n    if seed is not None:\n        rng = np.random.default_rng(seed)\n        signal = np.zeros(signal_length)\n        nonzero_indices = rng.choice(signal_length, num_nonzero, replace=False)\n        nonzero_values = 10*rng.standard_normal(num_nonzero)\n    else:\n        signal = np.zeros(signal_length)\n        nonzero_indices = np.random.choice(signal_length, num_nonzero, replace=False)\n        nonzero_values = 10*np.random.randn(num_nonzero)\n    signal[nonzero_indices] = nonzero_values\n    return signal\n\n# Make Feature 1 reproducible w.r.t. index number\nseed = int(index_no) if isinstance(index_no, (int, np.integer)) else 0\nsparse_signal = generate_signal(signal_length, num_nonzero, seed=seed)\n\n# Inject assignment-specific spike\nsparse_signal[10] = (int(index_no) % 10)*2 + 10\nif int(index_no) % 10 == 0:\n    sparse_signal[10] = float(np.random.randn(1) + 30)\nsparse_signal = sparse_signal / 5.0\n\n# Feature 2 (Gaussian-like)\nrng2 = np.random.default_rng(seed + 1)\nepsilon = rng2.normal(0, 15, signal_length)\n\n# Apply scalers\nf1_std   = standard_scale(sparse_signal)\nf1_minmax = minmax_scale(sparse_signal)\nf1_maxabs = maxabs_scale(sparse_signal)\n\nf2_std   = standard_scale(epsilon)\nf2_minmax = minmax_scale(epsilon)\nf2_maxabs = maxabs_scale(epsilon)\n\n# Plot originals\nplt.figure(figsize=(10,4))\nplt.stem(sparse_signal)\nplt.title('Feature 1 (Sparse) — Original')\nplt.xlabel('Index'); plt.ylabel('Value'); plt.grid(True)\nplt.show()\n\nplt.figure(figsize=(10,4))\nplt.stem(epsilon)\nplt.title('Feature 2 (Gaussian-like) — Original')\nplt.xlabel('Index'); plt.ylabel('Value'); plt.grid(True)\nplt.show()\n\n# Plot scaled variants for Feature 1\nplt.figure(figsize=(10,4))\nplt.stem(f1_std); plt.title('Feature 1 — Standard Scaled'); plt.grid(True); plt.show()\n\nplt.figure(figsize=(10,4))\nplt.stem(f1_minmax); plt.title('Feature 1 — Min-Max Scaled'); plt.grid(True); plt.show()\n\nplt.figure(figsize=(10,4))\nplt.stem(f1_maxabs); plt.title('Feature 1 — Max-Abs Scaled'); plt.grid(True); plt.show()\n\n# Plot scaled variants for Feature 2\nplt.figure(figsize=(10,4))\nplt.stem(f2_std); plt.title('Feature 2 — Standard Scaled'); plt.grid(True); plt.show()\n\nplt.figure(figsize=(10,4))\nplt.stem(f2_minmax); plt.title('Feature 2 — Min-Max Scaled'); plt.grid(True); plt.show()\n\nplt.figure(figsize=(10,4))\nplt.stem(f2_maxabs); plt.title('Feature 2 — Max-Abs Scaled'); plt.grid(True); plt.show()\n"},
    {"cell_type": "markdown", "metadata": {}, "source": "### Your choice & justification (write succinctly)\n- **Feature 1 (sparse spikes + many exact zeros):** _<choose one: MaxAbs / MinMax / Standard>_  \n  - _<justify: preserving zeros, scale of spikes, sign, etc.>_\n- **Feature 2 (Gaussian-like noise with mean≈0, high variance):** _<choose one: Standard / MinMax / MaxAbs>_  \n  - _<justify: preserving distribution shape, zero-mean/unit-variance, etc.>_"},

    {"cell_type": "markdown", "metadata": {}, "source": "## Appendix — References (for your reading)\n- scikit-learn preprocessing: <https://scikit-learn.org/stable/modules/preprocessing.html>  \n- Introduction to sparsity in signal processing: <https://eeweb.engineering.nyu.edu/iselesni/lecture_notes/sparsity_intro/sparse_SP_intro.pdf>  \n- scikit-learn LinearRegression: <https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html>"}
  ]
}
